{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# problem worked on: AI Hack Tunisia #6 - Predictive analytics challenge #3\n",
    "# url of the problem: https://zindi.africa/competitions/ai-hack-tunisia-6-predictive-analytics-challenge-3/leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")\n",
    "t_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used methods in this notebook:\n",
    "# data preparation: OneHotEncoding\n",
    "# Feature selection: using wrapper : exhaustive feature selection\n",
    "# ESTIMATORS = {\n",
    "#    \"Extra trees\": ExtraTreesRegressor(n_estimators=10,\n",
    "#                                       max_features=1,     # Out of 20000\n",
    "#                                       random_state=0),\n",
    "#    \"K-nn\": KNeighborsRegressor(),                          # Accept default parameters\n",
    "#    \"Linear regression\": LinearRegression(),\n",
    "#    \"Ridge\": RidgeCV(),\n",
    "#    \"Lasso\": Lasso(),\n",
    "#    \"ElasticNet\": ElasticNet(random_state=0),\n",
    "#    \"RandomForestRegressor\": RandomForestRegressor(max_depth=4, random_state=2),\n",
    "#    \"Decision Tree Regressor\":DecisionTreeRegressor(max_depth=5),\n",
    "#    \"MultiO/P GBR\" :GradientBoostingRegressor(),\n",
    "#    \"MultiO/P AdaB\" :AdaBoostRegressor(),\n",
    "#    \"GBR\" : GradientBoostingRegressor(),\n",
    "#     \"SVR\" : SVR(),\n",
    "#     \"xgb\": xgboost.XGBRegressor(n_estimators=100, learning_rate=0.1, gamma=0, subsample=0.75, colsample_bytree=1, max_depth=7)\n",
    "# }\n",
    "# will work with all these estimators for feature selection and modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('id', axis=1)\n",
    "t_df = t_df.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)\n",
    "t_df = pd.get_dummies(t_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('CTR_CATEGO_X_N', axis=1) # this column does not exist on test df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, Lasso, ElasticNet\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection: \n",
    "# 1- working with wrappers: \n",
    "# Step forward feature selection, \n",
    "# Step backwards feature selection and \n",
    "# Exhaustive feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18789, 98), (6264, 98))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# OneHotEncoding: \n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "# Splitting data: \n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    df.drop(labels=['target'], axis=1),\n",
    "    df['target'],\n",
    "    test_size=0.25,\n",
    "    random_state=41)\n",
    "\n",
    "# remove highly correlated features: \n",
    "correlated_features = set()\n",
    "correlation_matrix = df.corr()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "\n",
    "\n",
    "train_features.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "test_features.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "\n",
    "train_features.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step forward feature selection:\n",
    "# KNNRegressor with r2_score evaluation\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import r2_score\n",
    "methods = [KNeighborsRegressor, \n",
    "           XGBRegressor,\n",
    "           SVR,\n",
    "           DecisionTreeRegressor,\n",
    "           LinearRegression, \n",
    "           RidgeCV,\n",
    "           Lasso,\n",
    "           ElasticNet,\n",
    "           ExtraTreesRegressor, \n",
    "           RandomForestRegressor, \n",
    "           GradientBoostingRegressor, \n",
    "           AdaBoostRegressor]\n",
    "\n",
    "feature_selector = SequentialFeatureSelector(KNeighborsRegressor(),\n",
    "           k_features=98,\n",
    "           forward=True,\n",
    "           verbose=2,\n",
    "           scoring='r2',\n",
    "           cv=4)\n",
    "\n",
    "# features = feature_selector.fit(np.array(train_features.fillna(0)), train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_features= train_features.columns[list(features.k_feature_idx_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.7, gamma=0,\n",
       "             importance_type='gain', learning_rate=0.03, max_delta_step=0,\n",
       "             max_depth=7, min_child_weight=4, missing=None, n_estimators=500,\n",
       "             n_jobs=1, nthread=4, objective='reg:linear', random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=1,\n",
       "             subsample=0.7, verbosity=1)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run with XGBRegressor:\n",
    "# df = df[filtered_features]\n",
    "df.shape\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.20, random_state=0)\n",
    "xgb = XGBRegressor(colsample_bytree=0.7,learning_rate=0.03,max_depth=7,min_child_weight=4,\n",
    "n_estimators=500,\n",
    "nthread=4,\n",
    "objective='reg:linear',\n",
    "silent=1,\n",
    "subsample=0.7)\n",
    "xgb.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = xgb.predict(X_train)\n",
    "y_pred_test = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " xgboost train set: 4.38%\n",
      " xgboost test set : 5.81%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "mse_train = sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "mse_test = sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "print(\" xgboost train set: %.2f%%\" %(mse_train))\n",
    "print(\" xgboost test set : %.2f%%\" %(mse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-a97ef80af1c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmse_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmse_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" xgboost: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse_train\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "mse_train = sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "mse_test = sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "print(\" xgboost: %.2f%%\" %(mse_train*100))\n",
    "print(\" xgboost: %.2f%%\" %(mse_test*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
